{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dev/src\n",
      "train_data label rate  Label\n",
      "PortScan    101715\n",
      "BENIGN       81623\n",
      "Name: count, dtype: int64\n",
      "\n",
      "test_data label rate  Label\n",
      "PortScan    25429\n",
      "BENIGN      20406\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "Experiment to check how a Support Vector Machine model reacts against\n",
    "modifications in the synthetic testing datasets.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from data import load_csv_to_df\n",
    "import metrics\n",
    "from graph import get_flow_features_values, get_encoded_label\n",
    "from data import modify_portscan_attack_behavior,train_test_split_stratify,export_to_csv\n",
    "import configparser\n",
    "from sklearn import svm\n",
    "import os\n",
    "\n",
    "source_code_dir = '/home/dev/src'\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(source_code_dir+\"/config.ini\")\n",
    "\n",
    "\n",
    "model = svm.SVC()\n",
    "\n",
    "\n",
    "data = load_csv_to_df(\n",
    "    config[\"PARAMETERS\"][\"DataFolderPath\"] + config[\"PARAMETERS\"][\"TrainFile\"])\n",
    "\n",
    "# test = load_csv_to_df(\n",
    "#     config[\"PARAMETERS\"][\"DataFolderPath\"] + config[\"PARAMETERS\"][\"EvalFile\"])\n",
    "\n",
    "# 데이터 셋 분리 \n",
    "train,test  = train_test_split_stratify(data,0.3)\n",
    "\n",
    "print('train_data label rate',train[' Label'].value_counts())\n",
    "print()\n",
    "print('test_data label rate',test[' Label'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\"BENIGN\": 0, \"PortScan\": 1}\n",
    "labels = [\"BENIGN\", \"MALIGN\"]\n",
    "\n",
    "flow_features = [\" Flow Duration\"]\n",
    "\n",
    "# flow_features = [\n",
    "#     \" Average Packet Size\",\n",
    "#     \" Flow IAT Mean\",\n",
    "#     \" Flow Duration\",\n",
    "# ]\n",
    "\n",
    "flow_features = [    \n",
    "    \" Flow Duration\",\n",
    "    \" Flow Packets/s\",\n",
    "    \" Flow IAT Mean\",\n",
    "    \" Fwd IAT Mean\",\n",
    "    \" Bwd IAT Mean\",]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic noise를 위한 영역\n",
    "# test = modify_portscan_attack_behavior(test)\n",
    "\n",
    "# train vectors\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "    x_train.append(get_flow_features_values(row, flow_features))\n",
    "    y_train.append(get_encoded_label(\n",
    "        row, labels_map))\n",
    "\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "# test vectors\n",
    "x_test = []\n",
    "y_test = []\n",
    "for _, row in test.iterrows():\n",
    "    x_test.append(get_flow_features_values(row, flow_features))\n",
    "    y_test.append(get_encoded_label(\n",
    "        row, labels_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([183338, 5]), torch.Size([183338]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Evaluating model...\n",
      "0.6827533544234755\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "print(\"Training model...\")\n",
    "model.fit(x_train, y_train)\n",
    "print(\"Evaluating model...\")\n",
    "\n",
    "print(model.score(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F1 Score': 0.613184949124837,\n",
       " 'Accuracy': 0.6827533544234755,\n",
       " 'Recall': 0.6440126534807947,\n",
       " 'Precision': 0.8130585472627172}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "metrics.compute_metrics(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
